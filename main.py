import os
import base64
import logging
import json
import re
import asyncio
import random
from datetime import datetime, timedelta, timezone
from telethon import TelegramClient
from telethon.tl.functions.messages import GetHistoryRequest

# ========== é…ç½® ==========
api_id_str = os.getenv("API_ID")
api_hash = os.getenv("API_HASH")
session_b64 = os.getenv("SESSION_B64")

if not all([api_id_str, api_hash, session_b64]):
    raise ValueError("âŒ ç¼ºå°‘ç¯å¢ƒå˜é‡ï¼šAPI_IDã€API_HASH æˆ– SESSION_B64")

api_id = int(api_id_str)

# Decode SESSION_B64 to get the actual session binary data
session_file_path = "session.session"
with open(session_file_path, "wb") as session_file:
    session_file.write(base64.b64decode(session_b64))

# ========== æ—¥å¿—é…ç½® ==========
logging.basicConfig(level=logging.INFO,
    format='%(asctime)s - %(message)s',
    handlers=[logging.FileHandler("log.txt"), logging.StreamHandler()]
)

# åŸå§‹ç¾¤ç»„é“¾æ¥ï¼ˆå¯å«é‡å¤ï¼‰
raw_group_links = [
    'https://t.me/ConfigsHUB2',
    'https://t.me/config_proxy',
    'https://t.me/free_outline_keys',
    'https://t.me/freenettir',
    'https://t.me/v2ray_configs_pool',
    'https://t.me/VPN365R',
    'https://t.me/DailyV2RY',
    'https://t.me/Trick_mobil',
    'https://t.me/vpnplusee_free',
    'https://t.me/daily_configs',
    'https://t.me/oneclickvpnkeys',
    'https://t.me/V2All',
    'https://t.me/Outline_FreeKey',
    'https://t.me/V2ranNG_vpn',
    'https://t.me/v2rey_grum',
]

# å»é‡å¤„ç†ï¼Œå¹¶è®°å½•é‡å¤é¡¹
group_links = []
seen = set()
for link in raw_group_links:
    if link not in seen:
        group_links.append(link)
        seen.add(link)
    else:
        logging.warning(f"é‡å¤ç¾¤ç»„é“¾æ¥å·²å¿½ç•¥ï¼š{link}")

# åŒ¹é…é“¾æ¥çš„æ­£åˆ™è¡¨è¾¾å¼
url_pattern = re.compile(r'(vmess://[^\s]+|ss://[^\s]+|trojan://[^\s]+|vless://[^\s]+|tuic://[^\s]+|hysteria://[^\s]+|hysteria2://[^\s]+)', re.IGNORECASE)

# ========== è§£æèŠ‚ç‚¹ ==========
def parse_vmess_node(node, index):
    try:
        raw = base64.b64decode(node[8:])
        if not raw:
            return None
        conf = json.loads(raw)
        return {
            "name": f"vmess_{index}",
            "type": "vmess",
            "server": conf["add"],
            "port": int(conf["port"]),
            "uuid": conf["id"],
            "alterId": int(conf.get("aid", 0)),
            "cipher": "auto",
            "tls": conf.get("tls", "none") == "tls",
        }
    except Exception as e:
        logging.debug(f"è§£æ vmess å¤±è´¥: {e}")
        return None

# å…¶ä»–è§£æå‡½æ•°åŒæ ·ä¿æŒä¸å˜ï¼ˆçœç•¥ï¼‰

# ========== ç”Ÿæˆè®¢é˜…æ–‡ä»¶ ==========
async def generate_subscribe_file(nodes):
    try:
        # ç”Ÿæˆ base64 ç¼–ç è®¢é˜…
        joined_nodes = "\n".join(nodes)
        encoded = base64.b64encode(joined_nodes.encode()).decode()
        with open("sub", "w", encoding="utf-8") as f:
            f.write(encoded)
        logging.info("ğŸ‰ è®¢é˜…æ–‡ä»¶ç”Ÿæˆå®Œæ¯•")
    except Exception as e:
        logging.error(f"ç”Ÿæˆè®¢é˜…å¤±è´¥: {e}")

# ========== é”™è¯¯å¤„ç†ä¸é‡è¯•æœºåˆ¶ ==========
MAX_RETRIES = 3
RETRY_DELAY = 2  # æ¯æ¬¡é‡è¯•çš„å»¶è¿Ÿæ—¶é—´ï¼Œå•ä½ç§’

async def fetch_with_retries(fetch_function, *args, **kwargs):
    """æ·»åŠ é‡è¯•æœºåˆ¶ï¼Œå¤„ç†ç¬æ—¶ç½‘ç»œé—®é¢˜"""
    for attempt in range(MAX_RETRIES):
        try:
            return await fetch_function(*args, **kwargs)
        except Exception as e:
            if attempt < MAX_RETRIES - 1:
                delay = random.uniform(RETRY_DELAY, RETRY_DELAY * 2)  # éšæœºå»¶è¿Ÿ
                logging.debug(f"ç¬¬{attempt + 1}æ¬¡é‡è¯•å¤±è´¥: {e}ï¼Œç­‰å¾… {delay:.2f} ç§’")
                await asyncio.sleep(delay)
            else:
                logging.error(f"é‡è¯•å¤±è´¥: {e}")
                raise  # å¦‚æœé‡è¯•ç”¨å°½ï¼ŒæŠ›å‡ºå¼‚å¸¸

# ========== æŠ“å– Telegram æ¶ˆæ¯ ==========
async def fetch_messages_for_group(client, link, since):
    try:
        entity = await client.get_entity(link)
        history = await client(GetHistoryRequest(
            peer=entity,
            limit=100,
            offset_date=None,
            offset_id=0,
            max_id=0,
            min_id=0,
            add_offset=0,
            hash=0
        ))
        # ç­›é€‰å‡ºæ—¶é—´èŒƒå›´å†…çš„æ¶ˆæ¯
        messages = [msg for msg in history.messages if msg.date >= since]
        return link, messages
    except Exception as e:
        logging.error(f"æŠ“å– {link} æ¶ˆæ¯å¤±è´¥: {e}")
        return link, []

async def fetch_all_messages_with_rate_limit(client, group_links, since):
    tasks = [fetch_messages_for_group(client, link, since) for link in group_links]
    results = await asyncio.gather(*tasks)
    return results

# ========== ä¸»å‡½æ•° ==========
async def main():
    logging.info("ğŸš€ å¼€å§‹æŠ“å– Telegram èŠ‚ç‚¹")
    
    client = TelegramClient(session_file_path, api_id, api_hash)
    
    group_stats = {}  # ç”¨äºç»Ÿè®¡æ¯ä¸ªç¾¤ç»„çš„ç»“æœ

    try:
        # å¯åŠ¨å®¢æˆ·ç«¯
        await client.start()

        now = datetime.now(timezone.utc)
        all_links = set()

        # è®¾å®šæœ€å¤§æŠ“å–æ—¶é—´ï¼ˆæœ€å¤š23å°æ—¶ï¼‰
        max_hours = 24
        total_hours = 1  # åˆå§‹æŠ“å–æ—¶é—´ä¸º1å°æ—¶

        while total_hours <= max_hours:
            logging.info(f"ğŸ“… è®¾ç½®æŠ“å–æ—¶é—´èŒƒå›´: æœ€è¿‘ {total_hours} å°æ—¶")
            since = now - timedelta(hours=total_hours)
            group_stats.clear()  # æ¸…é™¤ä¹‹å‰çš„ç»Ÿè®¡æ•°æ®

            # å¹¶å‘æŠ“å–æ¯ä¸ªç¾¤ç»„çš„æ¶ˆæ¯
            results = await fetch_all_messages_with_rate_limit(client, group_links, since)

            # å¦‚æœæ²¡æœ‰ç¬¦åˆè¦æ±‚çš„èŠ‚ç‚¹ï¼Œè¿›å…¥ä¸‹ä¸€ä¸ªæ—¶é—´èŒƒå›´
            any_valid_node = False

            for link, messages in results:
                group_stats[link] = {"success": 0, "failed": 0}  # åˆå§‹åŒ–æ¯ä¸ªç¾¤ç»„çš„ç»Ÿè®¡

                for message in messages:
                    found = url_pattern.findall(message.message or '')
                    all_links.update(found)

                    # ç»Ÿè®¡æˆåŠŸçš„èŠ‚ç‚¹
                    for idx, node in enumerate(found):
                        if parse_vmess_node(node, idx) or parse_trojan_node(node, idx) or parse_vless_node(node, idx) or parse_ss_node(node, idx) or parse_tuic_node(node, idx) or parse_hysteria_node(node, idx) or parse_hysteria2_node(node, idx):
                            group_stats[link]["success"] += 1
                        else:
                            group_stats[link]["failed"] += 1

            if group_stats and any(stats["success"] > 0 for stats in group_stats.values()):
                any_valid_node = True  # å¦‚æœæœ‰ç¬¦åˆè¦æ±‚çš„èŠ‚ç‚¹ï¼Œåœæ­¢è°ƒæ•´æ—¶é—´èŒƒå›´
                break  # é€€å‡ºå¾ªç¯ï¼ŒæŠ“å–å·²å®Œæˆ

            total_hours += 1  # å¢åŠ æŠ“å–æ—¶é—´èŒƒå›´

        if not any_valid_node:
            logging.error("æ²¡æœ‰æŠ“å–åˆ°ç¬¦åˆè¦æ±‚çš„èŠ‚ç‚¹ï¼Œè¯·æ£€æŸ¥ç¾¤ç»„é…ç½®æˆ–ç½‘ç»œè¿æ¥ã€‚")
            return  # å¦‚æœæ²¡æœ‰ç¬¦åˆè¦æ±‚çš„èŠ‚ç‚¹ï¼Œåœæ­¢è„šæœ¬æ‰§è¡Œ

        logging.info(f"ğŸ”— æŠ“å–å®Œæˆï¼Œå…±æŠ“å– {len(all_links)} ä¸ªèŠ‚ç‚¹")
        unique_nodes = list(set(all_links))

        # ä»…ç”Ÿæˆ sub æ–‡ä»¶
        await generate_subscribe_file(unique_nodes)

        logging.info(f"ğŸ’¾ ä¿å­˜èŠ‚ç‚¹é…ç½®å®Œæˆï¼ŒèŠ‚ç‚¹æ•°ï¼š{len(unique_nodes)}")

        # è¾“å‡ºç¾¤ç»„ç»Ÿè®¡ä¿¡æ¯
        logging.info("ğŸ“Š æŠ“å–ç»Ÿè®¡:")
        for group_link, stats in group_stats.items():
            logging.info(f"{group_link}: æˆåŠŸ {stats['success']}ï¼Œå¤±è´¥ {stats['failed']}")

    except Exception as e:
        logging.error(f"ğŸ›‘ ç™»å½•å¤±è´¥: {e}")

if __name__ == "__main__":
    asyncio.run(main())
